{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# **California Housing Price Prediction - CRISP-DM Documentation**\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "#### 1. Business Understanding\n",
    "- **1.1** Determine Business Objectives  \n",
    "- **1.2** Assess Situation  \n",
    "- **1.3** Data Mining Goals and Research Questions  \n",
    "- **1.4** Produce Project Plan  \n",
    "\n",
    "#### 2. Data Understanding\n",
    "- **2.1** Collect Initial Data  \n",
    "- **2.2** Describe Data  \n",
    "- **2.3** Explore Data  \n",
    "- **2.4** Verify Data Quality  \n",
    "\n",
    "#### 3. Data Preparation\n",
    "- **3.1** Select Data  \n",
    "- **3.2** Clean Data  \n",
    "- **3.3** Construct Data  \n",
    "- **3.4** Integrate Data  \n",
    "- **3.5** Format Data  \n",
    "\n",
    "#### 4. Modeling\n",
    "- **4.1** Select Modeling Technique  \n",
    "  - Revisit: **3.5 Format Data**  \n",
    "- **4.2** Generate Test Design  \n",
    "- **4.3** Build Model  \n",
    "- **4.4** Assess Model  \n",
    "\n",
    "#### 5. Evaluation\n",
    "- **5.1** Evaluate Results  \n",
    "- **5.2** Review Process  \n",
    "- **5.3** Next Steps  \n",
    "\n",
    "#### 6. References\n",
    "\n",
    "\n",
    "\n",
    "# **Introduction**\n",
    "This project follows the **CRISP-DM (Cross-Industry Standard Process for Data Mining)** methodology to analyze and predict housing prices in California. The goal is to build a **machine learning model** that estimates housing prices based on demographic and geographic factors. The dataset used for this analysis comes from the **1990 California Census**, sourced from Kaggle [[1](https://www.kaggle.com/datasets/camnugent/california-housing-prices?resource=download)]. The California Census is part of the United States Census, a nationwide survey conducted every ten years to collect demographic, economic, and housing data.\n",
    "\n",
    "Since the dataset is publicly available for educational purposes, there are no specific business requirements. However, the project aims to provide valuable insights into California’s real estate market for stakeholders such as **real estate investors, homebuyers, policymakers, and financial institutions**. \n",
    "\n",
    "This documentation will follow CRISP-DM’s structured approach, starting with **Business Understanding**, followed by **Data Understanding, Data Preparation, Modeling, and Evaluation**, leaving out Deployment.\n",
    "\n",
    "# **1. Business Understanding**\n",
    "\n",
    "**Objective:**\n",
    "Any successful project begins with a solid understanding of the customer’s needs. Business understanding aims to establish the following:\n",
    "\n",
    "1. Define business goals and objectives – Clarify what the customer truly wants to achieve.\n",
    "2. Evaluate the current situation – Assess available resources, requirements, potential risks, and constraints.\n",
    "3. Analyze financial implications – Conduct a cost-benefit analysis to determine feasibility.\n",
    "4. Develop a project plan – Outline project phases, timelines, required resources, inputs, outputs, and dependencies.\n",
    "\n",
    "Since for this project, there is no customer involved, this section will be very thin, and is more from a general perspective.\n",
    "\n",
    "## **1.1 Determine Business Objectives**\n",
    "\n",
    "\n",
    "### **Background**\n",
    "The dataset originates from the **1990 California Census** and is provided by Kaggle. The California Census is a statewide demographic survey conducted as part of the United States Census. The 1990 California Census was a subset of the 1990 U.S. Census, which gathered population, economic, and housing data across the state [[2](https://data.census.gov/profile/California?g=040XX00US06)].\n",
    "\n",
    "\n",
    "### **Business Objectives**\n",
    "- The **primary objective** is to create a predictive model that estimates house prices based on available census data.\n",
    "\n",
    "- Additional considerations include:\n",
    "  - Identifying the most influential factors affecting house prices.\n",
    "  - Evaluating the impact of geographic location on housing prices.\n",
    "\n",
    "### **Business Success Criteria**\n",
    "A successful model should:\n",
    "- Provide **interpretable insights** into housing price drivers.\n",
    "- Demonstrate **reasonable predictive performance**.\n",
    "- Handle **data quality issues** effectively.\n",
    "- Be **achievable within a 5-week timeframe**, considering project constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## **1.2 Assess Situation**\n",
    "\n",
    "### **Available Resources**\n",
    "- **Personnel:** Me (Project Lead)\n",
    "- **Data:** California Housing Dataset from Kaggle (1990 Census Data).\n",
    "- **Computing Resources:** Jupyter Lab, Python Libraries, own machine.\n",
    "\n",
    "### **Requirements, Constraints & Assumptions**\n",
    "- **Data is from 1990**, meaning it may not reflect today’s market trends.\n",
    "- **No deep learning techniques allowed** as per project guidelines.\n",
    "- **Project duration is 5 weeks**, requiring efficient execution.\n",
    "- **Limited computational resources**, necessitating lightweight models.\n",
    "\n",
    "### **Risks & Contingencies**\n",
    "| **Risk** | **Mitigation Plan** |\n",
    "|----------|---------------------|\n",
    "| Computational limitations | Select efficient models and optimize complexity. |\n",
    "| Time constraints | Prioritize key tasks, avoid unnecessary complexity. |\n",
    "\n",
    "### **Costs and Benefits**\n",
    "**Costs:**\n",
    "- Time and effort required for data preparation and model tuning.\n",
    "- Computational resources necessary for data processing and modeling.\n",
    "\n",
    "**Benefits:**\n",
    "From a business perspective, the benefits only shines if you use the algorithms to train on newer data.\n",
    "\n",
    "- A well-performing model can provide valuable insights for investors, policymakers, and homebuyer.\n",
    "- Helps understand key housing market trends in California.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Data Mining Goals and Research Questions\n",
    "\n",
    "For the **California Housing project**, our primary goal is to develop and compare machine learning models that can accurately predict the median house value for block groups in California. Specifically, we aim to:\n",
    "\n",
    "- **Preprocess and Clean the Data:**  \n",
    "  - Handle missing values, address scaling issues, and manage outliers as necessary.\n",
    "\n",
    "- **Feature Engineering:**  \n",
    "  - Identify and derive important features that capture key aspects of the data.\n",
    "\n",
    "- **Model Development:**  \n",
    "  - Build and compare machine learning models to determine which model best estimates house prices.\n",
    "\n",
    "## Performance Targets  \n",
    "\n",
    "Our performance targets are based on multiple regression models implemented on Kaggle. Most of these models use **Root Mean Square Error (RMSE)**, **Mean Absolute Error (MAE)**, and the **Coefficient of Determination (R²)** as evaluation metrics.  \n",
    "\n",
    "Based on their performances, we inferred a target that falls within the mid-range of observed models [[3](https://www.kaggle.com/code/awnishsingh123/california-housing-prices-prediction)], [[4](https://www.kaggle.com/code/faizanali999/california-housing-prices-eda-modeling)], [[5](https://www.kaggle.com/code/joshcrotty/california-housing-prices-randomforestregressor)], [[6](https://www.kaggle.com/code/muhammetipil/everything-on-linear-regression#Model-Evaluations-)], [[7](https://www.kaggle.com/code/varshitanalluri/multiple-linear-regression)]. These examples represent a subset of existing models, and there are likely others with both better and worse performance.\n",
    "\n",
    "A review of existing models suggests the following range of performance metrics:\n",
    "\n",
    "RMSE measures the average error magnitude. A target below 60,000 means that, on average, the model's predictions will deviate from actual prices by a significant but manageable amount. While not highly precise, it ensures the model captures general price trends. Here, a lower RMSE is better.\n",
    "- **highest RMSE:** 69,274 – Linear Regression [[3](https://www.kaggle.com/code/awnishsingh123/california-housing-prices-prediction#XGBoost-regression)]  \n",
    "- **lowest RMSE:** 48,144 – XGBoost [[3](https://www.kaggle.com/code/awnishsingh123/california-housing-prices-prediction#XGBoost-regression)]\n",
    "\n",
    "R² represents the proportion of variance explained by the model. A value of 0.7 indicates that the model explains 70% of the variation in housing prices, which is acceptable but leaves room for improvement. A higher R² is better.\n",
    "- **lowest R²:** 0.59 – Linear Regression [[7](https://www.kaggle.com/code/varshitanalluri/multiple-linear-regression)]  \n",
    "- **highest R²:** 0.826 – XGBoost [[3](https://www.kaggle.com/code/awnishsingh123/california-housing-prices-prediction#XGBoost-regression)]\n",
    "\n",
    "MAE gives the average absolute error in predictions. A target under 40,000 suggests that the model will, on average, misestimate prices by this amount, making it useful for broad price estimations but not for highly precise valuations. A lower MAE is better.\n",
    "- **highest MAE:** 51,201 – Linear Regression [[3](https://www.kaggle.com/code/awnishsingh123/california-housing-prices-prediction#XGBoost-regression)]\n",
    "- **lowest MAE:** 31,656 – XGBoost [[3](https://www.kaggle.com/code/awnishsingh123/california-housing-prices-prediction#XGBoost-regression)]  \n",
    "\n",
    "\n",
    "So based on Research, simple Linear Regressions perform the worst and XGBoost performs the best on this dataset. \n",
    "\n",
    "\n",
    "### **Our Performance Targets:**  \n",
    "\n",
    "To define our performance targets, we took the average of the highest and lowest observed values for each metric. This provides a balanced goal that is neither too optimistic nor too weak, ensuring the model is at least reasonably effective in predicting housing prices. \n",
    "\n",
    "Our targets are:  \n",
    "\n",
    "- **RMSE:** Less than **60,000** on the test set  \n",
    "\n",
    "- **R²:** At least **0.7**  \n",
    "\n",
    "- **MAE:** Less than **40,000**\n",
    "\n",
    "For regression models, there are also some more metrics like Mean Squared Error (MSE) or Adjusted R², there will be no actual benefit from using them, since there are only transformed/adjustes versions of the metrics we use [[8](https://www.analyticsvidhya.com/blog/2021/05/know-the-best-evaluation-metrics-for-your-regression-model/#h-types-of-regression-metrics)]. To have a clear overview, we just compare these three metrics.\n",
    "\n",
    "While these targets do not indicate a highly accurate model, they ensure a model that can provide **reasonably useful predictions** for the median price of a housing block group in California.\n",
    "\n",
    "\n",
    "\n",
    "## Key Research Questions  \n",
    "\n",
    "1. **Which features are most influential in determining California housing prices?**  \n",
    "2. **Which machine learning model provides the most accurate predictions?**\n",
    "3. **Can there be derived features that improve the model?**\n",
    "4. **Which scaling method is the best for this dataset?**\n",
    "\n",
    "---\n",
    "\n",
    "## **1.4 Produce Project Plan**\n",
    "\n",
    "#### **Planned Steps**\n",
    "We have 5 weeks to implement this project. The plan is already defined by the project assignment:\n",
    "\n",
    "##### Weeks 3.1–3.3\n",
    "\n",
    "- Business Understanding: Define assignment, read related works, deep dive into the assignment’s domain.\n",
    "- Data Understanding: Explore and analyse the dataset.\n",
    "- Data Preparation: Clean, pre-process, and transform the dataset to prepare it for modelling.\n",
    "\n",
    "##### Weeks 3.3–3.5\n",
    "\n",
    "- Modelling: Select, train, and refine machine learning model/s suitable for the problem.\n",
    "- Evaluation: Assess the model(s) with correct performance metrics and discuss the results.\n",
    "\n",
    "##### End of week 3.5\n",
    "- Hand-in summative deliverable! That is a documented notebook.\n",
    "\n",
    "#### **Assessment of Tools & Techniques**\n",
    "- **Exploratory Data Analysis (EDA):** Pandas, Matplotlib, Seaborn, Scikit-Learn etc.\n",
    "- **Data Preprocessing:** Handling missing and wrong values, feature scaling.\n",
    "- **Modeling Techniques:**\n",
    "  - Supervised learning techniques, Regression techniques\n",
    "- **Evaluation Metrics:** MAE, RMSE, R² Score.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# **2. Data Understanding**\n",
    "\n",
    "## **2.1 Collect Initial Data**\n",
    "\n",
    "The dataset used for this analysis comes from the **1990 California Census**, sourced from Kaggle [[1](https://www.kaggle.com/datasets/camnugent/california-housing-prices?resource=download))].\n",
    "The raw data is stored in `data/raw/housing.csv`\n",
    "\n",
    "## **2.2 Describe Data**\n",
    "In this section we perform a high-level examination of the data.\n",
    "\n",
    "The dataset consists of 10 attributes and 20,640 records. The dataset format is CSV. The key fields include:\n",
    "\n",
    "Each row in the dataset represents a house in California. But most features are not describing the house, but the **block group/district** of that house, which is a geographical unit used in census data collection. Block groups typically contain between **600 and 3,000 people** and are smaller than census tracts, providing localized housing statistics. \n",
    "\n",
    "### **Dataset Features**\n",
    "1. **`longitude`**: A measure of how far **west** a house is; a higher value means farther west.\n",
    "2. **`latitude`**: A measure of how far **north** a house is; a higher value means farther north.\n",
    "3. **`housing_median_age`**: The **median age** of a house within a block; a lower number indicates newer buildings.\n",
    "4. **`total_rooms`**: The **total number of rooms** within a block.\n",
    "5. **`total_bedrooms`**: The **total number of bedrooms** within a block.\n",
    "6. **`population`**: The **total number of people** residing within a block.\n",
    "7. **`households`**: The **total number of households**, referring to groups of people living together in a housing unit within a block.\n",
    "8. **`median_income`**: The **median income** for households within a block (measured in **tens of thousands of US Dollars**).\n",
    "9. **`median_house_value`**: The **median house value** for households within a block (measured in **US Dollars**).\n",
    "10. **`ocean_proximity`**: A **categorical variable** indicating the house's **location relative to the ocean** (e.g., *\"Near Bay\"*, *\"Inland\"*, etc.).\n",
    "\n",
    "#### **Target Variable**:\n",
    "- Since we want to predict median house prices for households within a block, **`median_house_value`** will be the target variable.\n",
    "\n",
    "\n",
    "\n",
    "The dataset is numeric except for `ocean_proximity`, which is categorical. Initial statistics indicate the range of each feature. The data looks okay, to go on with further exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from IPython.display import display, SVG, HTML\n",
    "\n",
    "\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import Lasso, LassoLars, ElasticNet, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import sys # this is needed due to a problem installing xgboost in jupyter notebook\n",
    "!{sys.executable} -m pip install xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '../data/raw/housing.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Basic structure of the dataset\n",
    "print(\"Dataset Structure:\")\n",
    "df.info()\n",
    "\n",
    "# Number of rows and columns\n",
    "print(f\"\\nDataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "# First few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2.3 Explore Data\n",
    "\n",
    "In this section, we perform a detailed exploration of the data. Our objectives are to:\n",
    "\n",
    "- Understand the distributions of key numeric attributes.\n",
    "- Examine relationships between features, especially the relationship between predictors like median_income and the target variable median_house_value.\n",
    "- Investigate geospatial patterns using the longitude and latitude information.\n",
    "- Assess categorical differences by exploring the impact of ocean_proximity on housing prices.\n",
    "\n",
    "We already have a high-level description in Section 2.2. Now, let's take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 2.3.1 Distribution of the features\n",
    "We begin by examining the distributions of the main numeric attributes, excluding `longitude` and `latitude`. Because those two columns function primarily as coordinates, separate histograms would be less meaningful; a heatmap is more suitable to visualize geographic patterns. Likewise, for the categorical variable `ocean_proximity`, a bar or pie chart is more appropriate.\n",
    "\n",
    "From the summary statistics below, we can note the following:\n",
    "\n",
    "- The minimum value of **`households`** is **1**, indicating at least one block group with only a single household. Such an entry could be considered less representative, as larger block groups more reliably capture a “typical” median house value.\n",
    "- Several features—**`total_rooms`**, **`total_bedrooms`**, **`population`**, and **`households`**—show potential outliers, since their maximum values are considerably higher than their respective 75th-percentile values.\n",
    "- **`median_income`** also has notable upward outliers. Although its average is **3.53** and its 75th-percentile is **4.74**, the maximum climbs to **15**, indicating a small number of block groups with significantly higher incomes than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "print(\"\\nSummary Statistics:\")\n",
    "display(df.describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical features to visualize\n",
    "numerical_features = ['median_house_value', 'median_income', 'total_rooms', \n",
    "                      'total_bedrooms', 'population', 'households', 'housing_median_age']\n",
    "\n",
    "# Axis limits for each feature\n",
    "limits = {\n",
    "    'median_house_value': (0, 500000),\n",
    "    'median_income': (0, 16),\n",
    "    'total_rooms': (0, 40000),\n",
    "    'total_bedrooms': (0, 7000),\n",
    "    'population': (0, 40000),\n",
    "    'households': (0, 7000),\n",
    "    'housing_median_age': (0, 53)\n",
    "}\n",
    "\n",
    "# Automatically calculate the number of rows based on feature count\n",
    "num_features = len(numerical_features)\n",
    "num_cols = 3  # Keep 3 columns for a good layout\n",
    "num_rows = (num_features // num_cols) + (num_features % num_cols > 0)  # Auto-adjust rows\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(num_rows, num_cols, i)  # Adjust grid size dynamically\n",
    "    sns.histplot(data=df, x=feature, bins=50, kde=True)\n",
    "    plt.xlim(limits[feature])  # Apply custom x-axis limits\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "- All six numeric features appear **right-skewed**, consistent with our earlier observations in Section 2.3.1.  \n",
    "- **`median_house_value`** is capped at \\$500k, so any home valued above that amount is simply recorded as \\$500k. As a result, the model can only learn to predict values up to \\$500k.  \n",
    "- **`median_income`** is also capped, with an upper limit around 15, while **`housing_median_age`** is capped at 50.  \n",
    "- Despite the capping, **`housing_median_age`** otherwise follows a distribution close to normal, aside from a noticeable peak at the upper limit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_counts = df[\"ocean_proximity\"].value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(x=ocean_counts.index, y=ocean_counts.values, hue=ocean_counts.index, dodge=False, legend=False)\n",
    "\n",
    "# Add text annotations\n",
    "for i, count in enumerate(ocean_counts.values):\n",
    "    ax.text(i, count + 10, str(count), ha='center', fontsize=12)\n",
    "\n",
    "plt.xlabel(\"Ocean Proximity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Ocean Proximity Categories\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### 2.1.2 Correlations between features\n",
    "\n",
    "In this section, we examine how different features correlate with one another. This analysis is useful for:\n",
    "- Determining whether only a subset of features should be used, possibly improving model performance by removing redundant inputs.  \n",
    "- Identifying which features might serve as strong predictors for the target variable (house price).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns from the DataFrame\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = numeric_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix for Numeric Features in California Housing Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "- **`longitude` and `latitude`** exhibit strong negative correlation, which may stem from California’s geography; areas farther west (lower longitude) often lie slightly farther north (higher latitude). Consequently, it may be feasible to drop one of these coordinates if they are overly redundant.\n",
    "- **`total_rooms`, `total_bedrooms`, `population`,** and **`households`** are also strongly correlated, which is unsurprising given how each relates to the size and density of a block group. Hence, removing some of these attributes could reduce redundancy.\n",
    "- Regarding the target variable (**`median_house_value`**), **`median_income`** shows the strongest correlation. Most other features have a relatively low correlation with the target, suggesting a less direct impact on house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['median_income'], df['median_house_value'], alpha=0.5, color='teal')\n",
    "plt.xlabel('Median Income (in tens of thousands)')\n",
    "plt.ylabel('Median House Value (USD)')\n",
    "plt.title('Scatter Plot of Median Income vs. Median House Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "- **General upward trend:** as median income increases, the median house value tends to increase. This indicates that higher-income areas often have higher house values.\n",
    "- **Non-Linear Distribution:** Although there is a clear positive trend, the relationship isn’t perfectly linear. At higher incomes (beyond $100,000), house values do tend to approach the capped boundary, but there is still some spread below the cap.\n",
    "\n",
    "**Potential Modeling Considerations:**\n",
    "- The cap at \\$500,000 means any regression or predictive model may not distinguish differences above that value, so it might underestimate actual values in higher-end neighborhoods.\n",
    "- If modeling, we may want to consider a transformation (e.g., log transformation) to reduce skewness and stabilize variance, especially since house values are heavily skewed and capped [[9](https://journals.sagepub.com/doi/epub/10.1177/00045632211050531)].\n",
    "- The resulting model cannot predict values above \\$500,000 since it did not see data with higher values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=df['ocean_proximity'], y=df['median_house_value'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('House Value Distribution by Ocean Proximity')\n",
    "plt.xlabel('Ocean Proximity')\n",
    "plt.ylabel('Median House Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Since in the correlation matrix we did not take oceam proximity into account, here we plot the median house value and look into each category.\n",
    "Based on them we can say the following:\n",
    "- Inland Houses are mostly lower in price with some outliers\n",
    "- Island houses are very stable and have the highest prices\n",
    "    - It is weird that there is no house that is above 500k for a house on an island\n",
    "- Near Bay, <1H Ocean and near ocean are nearly equal and the prices are in the mid price range \n",
    "    - <1H ocean houses 75th percentile does not hit 500k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the California map image\n",
    "california_img = mpimg.imread('images/california.png')\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the scatter plot with appropriate settings\n",
    "sc = plt.scatter(\n",
    "    df['longitude'], df['latitude'], \n",
    "    alpha=0.4, s=df['population']/100,  # Adjusting size based on population\n",
    "    c=df['median_house_value'], cmap='jet', \n",
    "    edgecolor='k', linewidth=0.5\n",
    ")\n",
    "\n",
    "# Overlay the California map image\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\n",
    "\n",
    "# Add colorbar and labels\n",
    "plt.colorbar(sc, label='Median House Value')\n",
    "plt.xlabel('Longitude', fontsize=14)\n",
    "plt.ylabel('Latitude', fontsize=14)\n",
    "plt.title('Geospatial Distribution of Median House Value in California', fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(['Population Size'], loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "- Here we can see the same trend noted in our **`ocean_proximity`** analysis. Coastal and Bay Area locations on the west side of California tend to have **higher house prices**, as indicated by the warmer colors. Meanwhile, more **inland** regions—particularly the Central Valley and desert areas—display cooler colors, reflecting **lower median house values** overall. This visual reaffirms the significance of geographic location in determining housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 2.4 Verify Data Quality\n",
    "\n",
    "In this section, we evaluate the completeness, correctness, and consistency of the California housing dataset to identify potential issues before moving on to data preparation. These checks ensure that our subsequent analysis and modeling are built on a reliable foundation.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4.1 Coverage and Completeness\n",
    "\n",
    "- **Dataset Size:**  \n",
    "  The dataset contains **20,640 records** and **10 attributes**, covering a broad range of geographic locations and housing characteristics.\n",
    "- **Attribute Overview:**  \n",
    "  - **Numeric:** `longitude`, `latitude`, `housing_median_age`, `total_rooms`, `total_bedrooms`, `population`, `households`, `median_income`, `median_house_value`  \n",
    "  - **Categorical:** `ocean_proximity`\n",
    "\n",
    "We confirmed that all expected attributes are present and that the dataset aligns with the typical size for California block groups.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4.2 Missing Values Analysis\n",
    "\n",
    "We used `df.isnull().sum()` to check for missing entries in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values for each column\n",
    "missing_values = df.isnull().sum()\n",
    "display(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "\n",
    "- **Key Finding:**  \n",
    "  - total_bedrooms has **207 missing values** (≈1% of the dataset).  \n",
    "  - All other attributes have 0 missing values.\n",
    "\n",
    "#### Distribution of Missing Values Across ocean_proximity\n",
    "\n",
    "Because ocean_proximity can be a critical factor in housing prices, we investigated whether any of the missing total_bedrooms records were associated with \"ISLAND\" or other rare categories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per ocean proximity category\n",
    "missing_per_category = df[df['total_bedrooms'].isnull()]['ocean_proximity'].value_counts()\n",
    "\n",
    "# Count total records per ocean proximity category\n",
    "total_per_category = df['ocean_proximity'].value_counts()\n",
    "\n",
    "# Calculate percentage of missing values in each category\n",
    "missing_percentage = (missing_per_category / total_per_category * 100).fillna(0)\n",
    "\n",
    "# Create a summary DataFrame\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    \"Total Records\": total_per_category,\n",
    "    \"Missing Total Bedrooms\": missing_per_category.fillna(0).astype(int),\n",
    "    \"Percentage Missing (%)\": missing_percentage\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "display(missing_data_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "\n",
    "- **Result:**  \n",
    "  No missing total_bedrooms records are tied to \"ISLAND\". Hence, deleting these records would not disproportionately remove higher-priced island properties.\n",
    "\n",
    "#### Options for Addressing Missing Values\n",
    "\n",
    "1. **Deletion:**  \n",
    "   - We considered deleting the 207 records with missing total_bedrooms, as this represents only about 1% of the dataset.  \n",
    "   - **Risk:** If these records were important (e.g., island houses), their removal could bias our results. However, our checks confirmed none of these missing values are associated with the \"ISLAND\" category.\n",
    "\n",
    "2. **Imputation:**  \n",
    "   - To retain the most data, we opted to **impute** the missing total_bedrooms values.  \n",
    "   - **Method:** We will use the **median** of total_bedrooms. While more sophisticated methods (e.g., using total_rooms, population, and households to build a predictive model) are possible, the median imputation is straightforward and robust against outliers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 2.4.3 Duplicates and Data Consistency\n",
    "\n",
    "We checked for duplicate rows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "print(f\"Number of duplicate rows: {len(duplicate_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "- **Result:**\n",
    "  No duplicate rows were detected.\n",
    "\n",
    "**Data Consistency Checks:**\n",
    "- **Geographical Range:**  \n",
    "    The longitude and latitude values fall within expected bounds for California (see heatmap in 2.3)\n",
    "- **Attribute Ranges:**  \n",
    "    The ranges from housing_median_age, median_income and median_house_value all are capped. \n",
    "\n",
    "---\n",
    "\n",
    "### 2.4.4 Skewness and Outliers\n",
    "\n",
    "- **Skewed Distributions:**  \n",
    "  Histograms for several numeric features (total_bedrooms, population, total_rooms, etc.) indicate right-skewed distributions. This is common in housing data where a small number of areas have exceptionally high values.\n",
    "\n",
    "- **Potential Outliers:**  \n",
    "  It does not look like there are outliers, but also median house value is capped, so we do not really know if there are outliers in there or not. But for now we will continue with the assumption, that there are no outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4.5 Summary of Findings\n",
    "\n",
    "1. **Coverage:**  \n",
    "   The dataset is comprehensive for the California region, and all expected attributes are present.\n",
    "\n",
    "2. **Missing Values:**  \n",
    "   Only total_bedrooms contains missing entries (1% of the data). We decided to **impute** using the median.\n",
    "\n",
    "3. **Duplicates:**  \n",
    "   No duplicate records were found.\n",
    "\n",
    "4. **Data Consistency:**  \n",
    "   All numeric fields fall within reasonable ranges, and ocean_proximity is accurately represented as a categorical attribute.\n",
    "\n",
    "5. **Skewness and Outliers:**  \n",
    "   Some numeric features are right-skewed, and outliers in median_house_value reflect legitimate high-value properties. We will address skewness during data preparation if it impacts model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Based on this quality verification:\n",
    "- We will **impute** missing total_bedrooms values with the median in the **Data Cleaning** phase (Section 3.2).\n",
    "- We have no **outliers**.\n",
    "- We will **encode** ocean_proximity as a categorical feature.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# 3. Data Preperation\n",
    "\n",
    "In this section we will preprocess the data and ouptut a new version of the csv file in the `data/processed/` folder.\n",
    "\n",
    "## 3.1 Select Data\n",
    "\n",
    "#### Objective\n",
    "The goal of this step is to identify which attributes (columns) and records (rows) are most relevant for predicting `median_house_value`, while ensuring the dataset remains comprehensive and manageable for our analysis.\n",
    "\n",
    "#### Activities and Considerations\n",
    "\n",
    "1. **Attribute Selection**  \n",
    "   - At this stage, we have decided to retain all ten original attributes in our dataset:\n",
    "     - **Numeric:** `longitude`, `latitude`, `housing_median_age`, `total_rooms`, `total_bedrooms`, `population`, `households`, `median_income`, `median_house_value`  \n",
    "     - **Categorical:** `ocean_proximity`  \n",
    "   -  **Rationale for Keeping All Attributes:**\n",
    "        1. **Domain Relevance:**  \n",
    "            - Each attribute provides information about housing characteristics or location. For instance, `median_income` is known to be a strong predictor of house prices, and `ocean_proximity` offers critical location-based insights.\n",
    "        2. **Correlation Considerations:**  \n",
    "           - Although some features (e.g., `total_rooms`, `total_bedrooms`, `households`, `population`) are highly correlated, we are not excluding them at this stage. We may derive new features (such as `rooms_per_household` or `bedrooms_per_room`) during the **Construct Data** phase and later decide if removing any redundant attributes improves model performance.\n",
    "        \n",
    "        3. **Preserving Flexibility:**  \n",
    "           - By retaining all attributes now, we can evaluate different modeling strategies in later phases (e.g., **Modeling**). Techniques like regularization or tree-based methods can handle collinearity effectively, and feature importance metrics may guide which attributes to drop.\n",
    "        \n",
    "        4. **Categorical Variable Inclusion:**  \n",
    "           - We keep `ocean_proximity` despite it not appearing in the numeric correlation matrix because location is a major factor in housing prices. We will encode this feature in a future step to ensure it is usable in our modeling.\n",
    "\n",
    "2. **Record Selection**  \n",
    "   - **Full Dataset:**  \n",
    "     We are keeping all 20,640 records. Although 207 entries are missing values for `total_bedrooms`, this amounts to roughly 1% of the dataset. We decided to impute these rather than discard potentially informative records.\n",
    "   - **Rationale:**  \n",
    "     - The dataset is not excessively large, so we do not need to reduce its size for computational reasons.  \n",
    "     - We want to maintain broad coverage of different geographical areas and housing characteristics.\n",
    "\n",
    "3. **Exclusion of External Data**  \n",
    "   - **No Additional Sources:**  \n",
    "     We do not plan to merge external data (e.g., economic indicators or demographic datasets) as it does not align with our current project scope.  \n",
    "   - **Rationale:**  \n",
    "     The existing attributes sufficiently capture the primary factors affecting house values in California, and no other sources are deemed necessary at this time.\n",
    "\n",
    "4. **No Sampling Needed**  \n",
    "   - **Full Utilization:**  \n",
    "     Because the dataset is already of a manageable size, we will not perform downsampling or upsampling.  \n",
    "   - **Rationale:**  \n",
    "     - Retaining all records maximizes the information available for training and testing our predictive models.\n",
    "\n",
    "--- \n",
    "\n",
    "## 3.2 Clean Data\n",
    "\n",
    "In this phase, we address the primary data quality issue identified 2.4: missing values in the `total_bedrooms` column.\n",
    "\n",
    "### 3.2.1 Handling Missing Values\n",
    "\n",
    "We determined that **207 records** are missing values for `total_bedrooms`, which constitutes roughly **1%** of the dataset. To preserve as many records as possible, we opted to **impute** these missing values using the median of the non-missing entries in `total_bedrooms`. This approach retains the dataset’s integrity without significantly altering the distribution of values.\n",
    "\n",
    "Example of the imputation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping df for the raw data and create new dataframe df_processed for whole processing part\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Imputing median bedrooms for missing values\n",
    "median_bedrooms = df_processed['total_bedrooms'].median()\n",
    "\n",
    "\n",
    "df_processed['total_bedrooms'] = df_processed['total_bedrooms'].fillna(median_bedrooms)\n",
    "\n",
    "# Display missing values after imputation\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "display(df_processed.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "After this step, the `total_bedrooms` column has **no missing values**, and the dataset remains at **20,640 records**.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Imputation Completed:**  \n",
    "  Missing `total_bedrooms` values have been replaced with the median, ensuring no data loss.\n",
    "\n",
    "- **No Other Issues:**  \n",
    "  We have not identified additional cleaning requirements at this time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# 3.3 Construct Data\n",
    "\n",
    "**Objective:**  \n",
    "In this phase, we create new attributes (derived features) and transform existing data to enhance our analysis and modeling. Derived attributes can capture relationships not explicitly represented in the original dataset, while transformations (such as encoding categorical variables) ensure compatibility with machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3.1 Encoding Categorical Data\n",
    "\n",
    "Our dataset contains one categorical feature: **`ocean_proximity`**. Since most machine learning algorithms require numeric inputs, we must convert this column into a numeric representation. Below are common encoding methods and the rationale for selecting one for our use case [[10](https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/)]:\n",
    "\n",
    "1. **Label Encoding**  \n",
    "   - **Approach:** Assigns an integer value to each category (e.g., `\"NEAR BAY\" -> 0`, `\"INLAND\" -> 1`, `\"ISLAND\" -> 2\"`, etc.).  \n",
    "   - **Pros:** Very simple to implement and space-efficient.  \n",
    "   - **Cons:** Implies an ordinal relationship (e.g., 0 < 1 < 2), which typically does not make sense for nominal categories like `ocean_proximity`. This artificial ordering can mislead certain models (e.g., linear models).\n",
    "\n",
    "2. **Ordinal Encoding**  \n",
    "   - **Approach:** Similar to label encoding but explicitly ranks categories according to some notion of order (e.g., `\"ISLAND\" -> 1`, `\"NEAR BAY\" -> 2\"`, `\"INLAND\" -> 3\"`, etc.).  \n",
    "   - **Pros:** Useful if categories follow a natural order (e.g., \"small\", \"medium\", \"large\").  \n",
    "   - **Cons:** `ocean_proximity` has no inherent order, so ordinal encoding would be misleading.\n",
    "\n",
    "3. **One-Hot Encoding**  \n",
    "   - **Approach:** Creates separate binary indicator columns for each category. For instance, if there are categories `\"NEAR BAY\"`, `\"INLAND\"`, `\"ISLAND\"`, `\"NEAR OCEAN\"`, `\"<1H OCEAN\"`, each will become a new column with values 0 or 1.  \n",
    "   - **Pros:** Avoids imposing an order on nominal categories, and it is widely supported by many machine learning algorithms.  \n",
    "   - **Cons:** Increases the number of columns (dimensionality), which can be a concern for datasets with many categories.\n",
    "\n",
    "4. **Target or Frequency Encoding**  \n",
    "   - **Approach:** Replaces categories with aggregated statistics (e.g., mean target value, frequency counts).  \n",
    "   - **Pros:** Can reduce dimensionality while capturing category impact on the target.  \n",
    "   - **Cons:** Risk of data leakage (especially with target encoding) and may lose nuance in how categories differ.\n",
    "\n",
    "**Decision: One-Hot Encoding**  \n",
    "Given that `ocean_proximity` has no natural ordering and only a few distinct categories, **one-hot encoding** is the most straightforward and interpretable solution. It preserves the nominal nature of the data and avoids introducing artificial ordinal relationships. This process replaces the original `ocean_proximity` column with new columns such as `ocean_NEAR BAY`, `ocean_INLAND`, `ocean_ISLAND`, etc.\n",
    "\n",
    "\n",
    "**Implementation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'ocean_proximity'\n",
    "df_processed = pd.get_dummies(df_processed, columns=['ocean_proximity'], prefix='ocean')\n",
    "\n",
    "# Display the first few rows to confirm the new columns\n",
    "# True and False are equal to 1 and 0\n",
    "display(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.3.2 Creating Derived Features\n",
    "\n",
    "A common **feature engineering** technique involves deriving ratio- or density-based metrics from raw numeric features that are highly correlated or represent similar concepts. In our dataset, the attributes `total_rooms`, `total_bedrooms`, `population`, and `households` all describe the *size* or *count* of housing-related measures, but taken individually they do not account for *relative* differences among block groups. \n",
    "\n",
    "By **normalizing** these variables (for example, dividing by `households`), we capture proportions or densities that often have a clearer relationship to housing prices than raw totals [[11](https://medium.com/@evertongomede/feature-engineering-for-geospatial-data-navigating-the-spatial-frontier-4b6c8354eb2a)].\n",
    "\n",
    "Below are some ratio features we computed:\n",
    "\n",
    "1. **`rooms_per_household`**  \n",
    "   - **Calculation:** `total_rooms / households`  \n",
    "   - **Rationale:** Measures average **spaciousness**, reflecting how many rooms are available per household.\n",
    "\n",
    "2. **`bedrooms_per_room`**  \n",
    "   - **Calculation:** `total_bedrooms / total_rooms`  \n",
    "   - **Rationale:** Gauges the **layout or quality** of living space; a higher ratio could indicate smaller rooms or more emphasis on bedrooms.\n",
    "\n",
    "3. **`population_per_household`**  \n",
    "   - **Calculation:** `population / households`  \n",
    "   - **Rationale:** Captures **crowding** or **density**, highlighting how many people typically share a single household.\n",
    "\n",
    "**Implementation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features\n",
    "df_processed['rooms_per_household'] = df_processed['total_rooms'] / df_processed['households']\n",
    "df_processed['bedrooms_per_room'] = df_processed['total_bedrooms'] / df_processed['total_rooms']\n",
    "df_processed['population_per_household'] = df_processed['population'] / df_processed['households']\n",
    "\n",
    "# Display the first few rows to confirm the new derived columns\n",
    "display(df_processed[['rooms_per_household', 'bedrooms_per_room', 'population_per_household']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "\n",
    "#### **Distributions:**\n",
    "\n",
    "To see the distributions of these, we below plotted the histograms of the derived features. We use a log-transformation to have a more clear look at the distributions.\n",
    "\n",
    "We can see that also all these are right screwed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['rooms_per_household_log'] = np.log1p(df_processed['rooms_per_household'])\n",
    "df_processed['population_per_household_log'] = np.log1p(df_processed['population_per_household'])\n",
    "\n",
    "# Create figure with subplots: first row for original, second row for transformed\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "\n",
    "# Original distributions\n",
    "sns.histplot(df_processed['rooms_per_household'], ax=axes[0, 0], kde=True)\n",
    "axes[0, 0].set_title('Original: rooms_per_household')\n",
    "\n",
    "sns.histplot(df_processed['bedrooms_per_room'], ax=axes[0, 1], kde=True)\n",
    "axes[0, 1].set_title('Original: bedrooms_per_room')\n",
    "\n",
    "sns.histplot(df_processed['population_per_household'], ax=axes[0, 2], kde=True)\n",
    "axes[0, 2].set_title('Original: population_per_household')\n",
    "\n",
    "# Transformed distributions\n",
    "sns.histplot(df_processed['rooms_per_household_log'], ax=axes[1, 0], kde=True)\n",
    "axes[1, 0].set_title('Log-Transformed: rooms_per_household')\n",
    "\n",
    "sns.histplot(df_processed['bedrooms_per_room'], ax=axes[1, 1], kde=True)\n",
    "axes[1, 1].set_title('Log-Transformed: bedrooms_per_room (unchanged)')\n",
    "\n",
    "sns.histplot(df_processed['population_per_household_log'], ax=axes[1, 2], kde=True)\n",
    "axes[1, 2].set_title('Log-Transformed: population_per_household')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### **Correlations:**\n",
    "\n",
    "Below is a correlation matrix for the newly derived features alongside **`median_income`** and **`median_house_value`**. We observe that:\n",
    "\n",
    "- **`bedrooms_per_room`** and **`rooms_per_household`** exhibit notable correlations with both `median_income` and `median_house_value`.  \n",
    "- **`population_per_household`** shows comparatively weak correlation, but we retain it for completeness since it may still prove useful in certain models or interact with other features in meaningful ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of features to check, including the new derived ones\n",
    "features_to_check = [\n",
    "    'rooms_per_household',\n",
    "    'bedrooms_per_room',\n",
    "    'population_per_household',\n",
    "    'median_income',\n",
    "    'median_house_value'\n",
    "]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_processed[features_to_check].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation of Constructed Features with Income and House Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### 3.3.3 Scaling\n",
    "\n",
    "At this moment we will skip deciding on scaling for features, since we do not know what kind of model we will use and if it is even necessary or not.\n",
    "\n",
    "### 3.3.4 Summary\n",
    "\n",
    "1. **Categorical Encoding:**  \n",
    "   We chose **one-hot encoding** for `ocean_proximity` because it handles nominal categories without imposing an order, making it ideal for our use case.\n",
    "\n",
    "2. **Derived Attributes:**  \n",
    "   We introduced features like `rooms_per_household` and `bedrooms_per_room` to capture housing density and composition, potentially enhancing our predictive power.\n",
    "\n",
    "\n",
    "\n",
    "## 3.4 Integrate Data\n",
    "\n",
    "**Objective:**  \n",
    "Data integration involves combining information from multiple sources or tables to create a unified dataset. It can also include aggregation, where records are summarized to form new values (e.g., summing sales data by store).\n",
    "\n",
    "**Why We Can Skip This Step:**  \n",
    "- Our project currently relies on a single dataset for California housing, which already includes all necessary attributes (e.g., location, demographic, and housing variables).  \n",
    "- We have no additional tables or external data sources to merge, so there is no need to perform any further integration or aggregation at this stage.\n",
    "\n",
    "\n",
    "## 3.5 Format Data\n",
    "\n",
    "**Purpose:**  \n",
    "This step ensures that our prepared dataset is in the correct **syntactic** form for whichever modeling tools we plan to use. It involves reordering columns, confirming file formats, and applying any final adjustments that do not change the semantic meaning of the data.\n",
    "\n",
    "**Our Approach:**  \n",
    "- We have already encoded our categorical data and constructed new features, so there is no need for further transformations.  \n",
    "- The dataset is stored in a **DataFrame** that aligns with most machine learning frameworks, meaning no special column ordering or delimiter changes are required.  \n",
    "- We will **export** our processed DataFrame to a new CSV file (e.g., `data/preprocessed/california_housing_prepared.csv`) without further formatting changes.\n",
    "\n",
    "By finalizing our data in this manner, we ensure it is **ready for modeling** without altering its content or meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting processed df to 'data/processed' Folder \n",
    "df_processed.to_csv(\"../data/processed/housing_preprocessed.csv\", index=False)\n",
    "\n",
    "# Load the saved CSV\n",
    "df_loaded = pd.read_csv(\"../data/processed/housing_preprocessed.csv\")\n",
    "\n",
    "# check number of rows\n",
    "print(f\"🔹 Number of rows in loaded DataFrame: {df_loaded.shape[0]}\")\n",
    "\n",
    "# check first few rows\n",
    "print(\"\\n🔹 First few rows:\")\n",
    "display(df_loaded.head())\n",
    "df_loaded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## 4.1 Select Modeling Technique\n",
    "\n",
    "**Objective:**  \n",
    "We aim to identify one or more appropriate algorithms for predicting the continuous target variable `median_house_value`. Since housing price prediction involves estimating a numerical value rather than a category, it is a regression problem, and we will focus on regression-based techniques.\n",
    "\n",
    "#### Constraints and Considerations\n",
    "- **Supervised Learning:** We have a clearly defined target variable (`median_house_value`) for which labeled data is available.\n",
    "- **Data Size:** The dataset contains **20,640 records**\n",
    "- **Feature Importance:** From our correlation analysis, only a few features (e.g., `median_income`) strongly correlate with the target, suggesting that models which incorporate built-in feature selection or regularization might be beneficial.\n",
    "- **No Deep Learning:** We are not allowed to use Deep Learning techniques for this project.\n",
    "\n",
    "### 4.1.1 Using the Scikit-Learn Estimator Guide\n",
    "\n",
    "We reference the scikit-learn machine learning map(see image below) to select appropriate machine learning models [[12](https://scikit-learn.org/stable/machine_learning_map.html)]. Going with best practices recommended by the scikit-learn-team, will save us time. Here is a simplified walkthrough of the decision points:\n",
    "\n",
    "1. **More than 50 Samples?**  \n",
    "   - **Yes** (we have over 20k samples).\n",
    "\n",
    "2. **Predicting a Category?**  \n",
    "   - **No** (we are predicting a continuous value: `median_house_value`).\n",
    "\n",
    "3. **Predicting Quantity?**  \n",
    "   - **Yes** (this is a regression problem).\n",
    "\n",
    "4. **Less than 100k Samples?**  \n",
    "   - **Yes** (we only have about 20k records).  \n",
    "   - **Why Not SGD Regressor?**  \n",
    "     - Stochastic Gradient Descent (SGD) can be very effective for **large** datasets (often hundreds of thousands or millions of samples). With only ~20k samples, its advantages over other methods are less pronounced, and it can be more finicky to tune.\n",
    "\n",
    "5. **Few Features Are Important?**  \n",
    "   - **Yes** (our exploration showed a few strong predictors, while others may be less relevant).  \n",
    "   - **Recommended Models:**  \n",
    "     - **Lasso** and **ElasticNet** are good choices when only a few features are expected to be truly important. These methods use L1 or a combination of L1/L2 regularization, which can shrink irrelevant coefficients to zero, aiding interpretability and feature selection.\n",
    "\n",
    "#### Additional Model Considerations\n",
    "\n",
    "If Lasso or ElasticNet do not yield satisfactory performance or if we wish to explore alternatives, we can also evaluate the models for the `NO` path in step 5:\n",
    "- **Ridge Regression:** Uses L2 regularization (shrinks coefficients but does not force them to zero).  \n",
    "- **SVR (kernel=\"linear\" or \"rbf\"):** Can capture more complex relationships but often requires careful parameter tuning.  \n",
    "- **Ensemble Methods (e.g., Random Forest, XGBoost):** May handle non-linearities and interactions well, potentially at the cost of reduced interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(\"images/ml_map.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## 4.1.2 Lasso and ElasticNet: Modeling Assumptions and Explanations\n",
    "\n",
    "In this section, we focus on two regularized linear regression techniques—**Lasso** and **ElasticNet**—and explain their underlying principles, modeling assumptions, and why they are suitable for our housing price prediction problem. For further details, we refer to the scikit-learn supervised learning guide [[13](https://scikit-learn.org/stable/supervised_learning.html)]. The mathematical formulation of the models can also be found there, and we will not go into the mathematics of the models here.\n",
    "\n",
    "### Modeling Assumptions\n",
    "\n",
    "Both Lasso and ElasticNet assume:\n",
    "- A **linear relationship** between the predictors and the target variable.\n",
    "- That the errors (residuals) are independently and identically distributed.\n",
    "- That the input features are on a comparable scale (scaling is recommended).\n",
    "- They incorporate regularization (L1 for Lasso; a combination of L1 and L2 for ElasticNet) to prevent overfitting, particularly when only a subset of features is truly informative.\n",
    "- Since our dataset has a single output variable (`median_house_value`), we do not use the multi-task variants of these models.\n",
    "\n",
    "Based on this, we still need to scale our preprocessed dataset.\n",
    "\n",
    "### Lasso\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) minimizes the usual least squares error with an added L1 penalty on the coefficients. This penalty encourages sparsity by driving many coefficients to zero, effectively performing feature selection. \n",
    "\n",
    "This method is particularly attractive when we expect only a subset of features (such as `median_income`) to be strongly related to housing prices.\n",
    "\n",
    "### LARS Lasso\n",
    "\n",
    "LARS (Least Angle Regression) Lasso is an alternative algorithm for solving the Lasso problem. It provides the exact solution path as a function of the regularization parameter and is efficient in terms of computation. However, as noted on StackExchange, the differences between LARS and standard Lasso (typically solved by coordinate descent) are generally slight [[14](https://stats.stackexchange.com/questions/4663/least-angle-regression-vs-lasso#:~:text=Generally%2C%20there%20are%20tiny%20differences,LARS%20win%20the%20speed%20challenge)]. The choice between them often comes down to practical considerations—coordinate descent methods are usually faster and simpler to implement in modern settings. We are going to try both to see if LARS method or regular Lasso works better.\n",
    "\n",
    "> \"The 'no free lunch' theorems suggest that there are no a-priori distinctions between statistical inference algorithms... In practice then, it is best to try both and use some reliable estimator of generalisation performance to decide which to use in operation.\"  \n",
    "> — Dikran Marsupial, StackExchange [[14](https://stats.stackexchange.com/questions/4663/least-angle-regression-vs-lasso)]\n",
    "\n",
    "### ElasticNet\n",
    "\n",
    "ElasticNet combines L1 and L2 regularization, addressing some limitations of Lasso when predictors are highly correlated.\n",
    "\n",
    "This mix allows ElasticNet to retain the sparsity benefits of Lasso while also stabilizing the solution like Ridge Regression. It is useful when groups of correlated features are present, as it tends to select them together.\n",
    "\n",
    "### L1 and L2 Regularization Explained\n",
    "\n",
    "Here is a short explanation of L1 and L2 Regularization based on a blog post by neptune.ai [[15](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization)]. \n",
    "\n",
    "**L1 Regularization:**  \n",
    "In Lasso, the L1 penalty adds a cost equal to the sum of the absolute values of the coefficients. This encourages sparsity by driving many coefficients to zero, effectively selecting only the most important features.\n",
    "\n",
    "**L2 Regularization:**  \n",
    "In ElasticNet, an additional L2 penalty is included, which adds a cost proportional to the sum of the squared coefficients. This term helps to stabilize the model by preventing any one coefficient from becoming excessively large, without forcing them to zero.\n",
    "\n",
    "### Modeling Assumptions Summary\n",
    "\n",
    "- **Linearity:** Both techniques assume a linear relationship between inputs and the target.\n",
    "- **Feature Scaling:** They assume that features are on similar scales to ensure the regularization penalty works effectively.\n",
    "- **Regularization:** Lasso assumes that a sparse model (with many zero coefficients) is appropriate, while ElasticNet balances sparsity with coefficient stability.\n",
    "\n",
    "These assumptions generally hold in our dataset based on our data description and preparation phases. If the data had significant non-linearity or different error distributions, we might need to revisit our data preparation steps or consider alternative models.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## *Revisit: 3.5 Format Data*\n",
    "\n",
    "As expected, CRISP-DM is not a linear process. We can go back and forth between process to make sure it fits the business and data mining goal. In 4.1.2 we found out, that the models work best with scaled data. So here, we go back to data preperation phase to scale the data before we start to train our machine learning models.\n",
    "\n",
    "### Feature Transformation and Scaling\n",
    "\n",
    "According to Wikipedia's entry on Feature Scaling, scaling methods such as **Standard Scaling (z-score)**, **Min-Max Scaling**, and **Robust Scaling** are commonly used to normalize features [[16](https://en.wikipedia.org/wiki/Feature_scaling#Methods)]. This is especially important for **Lasso** and **ElasticNet**, which are linear models relying on regularization (L1/L2) and can be heavily influenced by features on very different scales.\n",
    "\n",
    "Below is a concise overview of these methods, plus a discussion on **log transformations** (which are not covered in the Wikipedia article but are widely used in practice).\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5.1 Common Scaling Methods\n",
    "\n",
    "\n",
    "#### 1. StandardScaler (z-score)\n",
    "- **Definition (Conceptual)**:  \n",
    "  Subtract the average (mean) of each feature from every data point, then divide by the standard deviation of that feature. This transformation centers the feature distribution at **0** and sets its spread to **1**.\n",
    "- **Advantages**:\n",
    "  - Widely used in **linear models** (e.g., Lasso, ElasticNet, Ridge).\n",
    "  - Makes coefficients more directly comparable across features.\n",
    "  - Often the default choice for algorithms sensitive to feature magnitude.\n",
    "- **Disadvantages**:\n",
    "  - **Sensitive to outliers**: A few extreme values can shift the mean and inflate the standard deviation.\n",
    "\n",
    "**Why StandardScaler over MinMax?**  \n",
    "- **MinMaxScaler** compresses all values into a [0, 1] range. If you have even a single extremely large or small value, **all other points get \"squeezed\"** into a narrow range.  \n",
    "- For the California Housing dataset, many features are right-skewed (e.g., total_rooms, population) or truncated (e.g., median_house_value capped at \\$500k). **StandardScaler** tends to handle these truncations more gracefully, whereas MinMaxScaler can overly compress the majority of data if there is even a small fraction of high values [[17](https://vitalflux.com/minmaxscaler-standardscaler-python-examples/#:~:text=Similarly%2C%20when%20dealing%20with%20non,the%20algorithm%20requires%20standardized%20features.)].\n",
    "\n",
    "\n",
    "#### 2. MinMaxScaler\n",
    "- **Definition (Conceptual)**:  \n",
    "  Subtract the smallest (minimum) value of a feature from each data point, then divide by the overall range (maximum minus minimum). This maps all values into a fixed interval (commonly [0, 1]).\n",
    "- **Advantages**:\n",
    "  - Simple interpretation: all features lie within the same fixed interval.\n",
    "  - Useful for algorithms that need bounded inputs (e.g., certain neural networks).\n",
    "- **Disadvantages**:\n",
    "  - **Highly sensitive to outliers**: a single extreme value can drastically compress the scaling for typical data points.\n",
    "  - If a feature’s maximum is much larger than most observations, those observations end up bunched near 0.\n",
    "\n",
    "\n",
    "#### 3. RobustScaler\n",
    "- **Definition (Conceptual)**:  \n",
    "  Instead of subtracting the mean, RobustScaler subtracts the **median** of each feature; instead of dividing by the standard deviation, it divides by the **interquartile range (IQR)** (the range between the 25th and 75th percentiles). \n",
    "- **Advantages**:\n",
    "  - Less sensitive to genuine outliers, because the median and IQR are more robust measures of central tendency and spread.\n",
    "- **Disadvantages**:\n",
    "  - If the dataset has many values at an artificial cap (e.g., \\$500k for `median_house_value`), these may not act like traditional outliers, and RobustScaler might not fully solve skewness or truncation issues.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5.2 Log Transformation\n",
    "\n",
    "While Wikipedia's feature scaling article focuses on the above numerical scalers, **log transformations** are a **commonly used technique** to deal with **highly skewed distributions** in regression problems. Why this is the case can be read in this paper from Robert M. West 'Best practice in statistics: The use of log transformation' [[18](https://journals.sagepub.com/doi/epub/10.1177/00045632211050531)] \n",
    "\n",
    "#### What is a Log Transformation?\n",
    "A **log transform** typically replaces a feature \\( X \\) with \\( \\log(1 + X) \\) (the `+1` is to avoid taking the log of zero). This transformation:\n",
    "1. **Compresses** very large values more aggressively than smaller values.\n",
    "2. **Reduces right skewness**, bringing distributions closer to normal.\n",
    "\n",
    "#### Why Do We Need It Here?\n",
    "- The California Housing dataset includes variables like `total_rooms`, `population`, `households`, and `median_income`, which are **heavily right-skewed**.\n",
    "- By applying `np.log1p(...)`, we **\"unskew\"** these features, making them more suitable for linear models. This can improve:\n",
    "  - **Model performance** (coefficients fit more reliably).\n",
    "  - **Stability** of the training process (less sensitivity to extreme values).\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5.3 How to engineer each feature\n",
    "\n",
    "\n",
    "Below is an explanation on how to handle each feature before applying linear models like **Lasso** or **ElasticNet**. The primary goal is to deal with **skewed distributions** and **scale disparities** between features.\n",
    "\n",
    "Based on our review of the data's distributions and values in Sections 2 and 3, we can now determine the proper transformation and scaling for each attribute.\n",
    "\n",
    "### Group 1: Log Transformation + Standard Scaling\n",
    "These features exhibit a wide range of values and benefit from a logarithmic adjustment before being standardized:\n",
    "- **total_rooms**\n",
    "- **total_bedrooms**\n",
    "- **population**\n",
    "- **households**\n",
    "- **median_income**\n",
    "- **rooms_per_household** (derived as total_rooms/households)\n",
    "- **population_per_household** (derived as population/households)\n",
    "\n",
    "### Group 2: Standard Scaling Only\n",
    "These features already exist on a controlled or naturally bounded scale, so they are adjusted solely for consistency:\n",
    "- **longitude, latitude**\n",
    "- **housing_median_age**\n",
    "- **bedrooms_per_room** (a ratio naturally between 0 and 1)\n",
    "\n",
    "### Group 3: No Transformation\n",
    "Binary dummy variables are already appropriately formatted:\n",
    "- **ocean_<1H OCEAN, ocean_INLAND, ocean_ISLAND, ocean_NEAR BAY, ocean_NEAR OCEAN**\n",
    "\n",
    "### Target Variable\n",
    "- **median_house_value**  \n",
    "  Since this is also right skewed, we will transform the data, and after predicting, we will reverse the transformation.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Implementation for transformation + Scaling\n",
    "\n",
    "We will here only include the function for transforming and scaling. The actuall scaling on the data will happen later after splitting the data, to avoid data leakage [[19](https://machinelearningmastery.com/data-preparation-without-data-leakage/)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test, scaler_type=\"minmax\", log_scale_cols=None, scale_only_cols=None):\n",
    "    \"\"\"\n",
    "    Applies log transformation and scaling to features only (not targets).\n",
    "    \"\"\"\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    # Apply log transform if specified\n",
    "    if log_scale_cols:\n",
    "        for col in log_scale_cols:\n",
    "            X_train_scaled[col] = np.log1p(X_train_scaled[col])\n",
    "            X_test_scaled[col] = np.log1p(X_test_scaled[col])\n",
    "    \n",
    "    # Determine columns to scale\n",
    "    cols_to_scale = []\n",
    "    if log_scale_cols:\n",
    "        cols_to_scale.extend(log_scale_cols)\n",
    "    if scale_only_cols:\n",
    "        cols_to_scale.extend(scale_only_cols)\n",
    "    \n",
    "    # Choose and apply scaler if needed\n",
    "    if len(cols_to_scale) > 0 and scaler_type is not None:\n",
    "        if scaler_type == \"minmax\":\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaler_type == \"standard\":\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaler type. Choose 'minmax' or 'standard'.\")\n",
    "            \n",
    "        X_train_scaled[cols_to_scale] = scaler.fit_transform(X_train_scaled[cols_to_scale])\n",
    "        X_test_scaled[cols_to_scale] = scaler.transform(X_test_scaled[cols_to_scale])\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, None\n",
    "    \n",
    "def transform_target(y_train, y_test, transform=False):\n",
    "    \"\"\"\n",
    "    Optionally apply log transformation to target variables.\n",
    "    \"\"\"\n",
    "    if transform:\n",
    "        y_train_transformed = np.log1p(y_train)\n",
    "        y_test_transformed = np.log1p(y_test)\n",
    "        return y_train_transformed, y_test_transformed\n",
    "    else:\n",
    "        return y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## 4.2 Generate Test Design\n",
    "\n",
    "**Objective:**  \n",
    "We need to define how we will evaluate our models’ performance in a reliable and unbiased manner. Since we are dealing with a regression task and have a moderate dataset (~20k records), we will split the data into **training** and **test** sets, and use **cross-validation** on the training set for hyperparameter tuning.\n",
    "\n",
    "### Train/Test Split\n",
    "\n",
    "- **Split Ratio:** We will use an **80/20** split, where 80% of the data is for training (and validation via cross-validation) and 20% is held out for final testing.\n",
    "- **Random State:** A fixed `random_state` (e.g., 42) ensures the split is reproducible.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "- **Method:** We will perform **5-fold cross-validation** on the training portion. This approach partitions the training data into 5 subsets (folds), trains on 4 folds, and validates on the remaining fold, iterating this process 5 times.\n",
    "- **Why 5 Folds?**  \n",
    "  - Provides a good balance between training size and validation stability.  \n",
    "  - More folds (e.g., 10) can increase the computational cost without necessarily offering significantly better performance estimates for a dataset of our size [[20](https://www.machinelearningmastery.com/k-fold-cross-validation/)].\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "During cross-validation, we will explore different hyperparameters using GridSearch. This ensures we select the best settings before we perform the final evaluation on the test set.\n",
    "\n",
    "### Final Test Evaluation\n",
    "\n",
    "- After deciding on the optimal hyperparameters, we will **retrain** the model on the entire training set (80%) and evaluate it on the **20%** test set.\n",
    "- **Performance Metric:** We will use **Mean Squared Error (MSE)** or **Root Mean Squared Error (RMSE)** to quantify predictive accuracy for the continuous target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_modeling = pd.read_csv(\"../data/processed/housing_preprocessed.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_modeling.drop(columns=[\"median_house_value\"])\n",
    "y = df_modeling[\"median_house_value\"]\n",
    "\n",
    "# Train/Test Split\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Keep a copy of raw data for the no-scaling experiment\n",
    "X_train_unscaled = X_train_raw.copy()\n",
    "X_test_unscaled = X_test_raw.copy()\n",
    "\n",
    "# Define columns for transformation\n",
    "log_scale_cols = [\n",
    "    \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \n",
    "    \"median_income\", \"rooms_per_household\", \"population_per_household\"\n",
    "]\n",
    "scale_only_cols = [\n",
    "    \"longitude\", \"latitude\", \"housing_median_age\"\n",
    "]\n",
    "\n",
    "# Apply different scaling approaches to features\n",
    "X_train_unscaled, X_test_unscaled, _ = scale_data(X_train_raw, X_test_raw, scaler_type=None)\n",
    "X_train_minmax, X_test_minmax, _ = scale_data(X_train_raw, X_test_raw, scaler_type=\"minmax\", \n",
    "                                                log_scale_cols=log_scale_cols, scale_only_cols=scale_only_cols)\n",
    "X_train_standard, X_test_standard, _ = scale_data(X_train_raw, X_test_raw, scaler_type=\"standard\", \n",
    "                                                  log_scale_cols=log_scale_cols, scale_only_cols=scale_only_cols)\n",
    "\n",
    "# Handle target transformation separately\n",
    "# For raw data we do not transform the target.\n",
    "y_train_raw, y_test_raw = transform_target(y_train_raw, y_test_raw, transform=False)\n",
    "# For scaled data, we apply log transformation to the target.\n",
    "y_train_transformed, y_test_transformed = transform_target(y_train_raw, y_test_raw, transform=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# 4.3 Build Model\n",
    "\n",
    "In this section, we build and evaluate several linear models on our prepared dataset. Our goal is to generate one or more models using the techniques outlined in Section 4.1, document the parameter settings and rationale, and assess their initial performance. This serves as the basis for subsequent refinement and hyperparameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3.1 Initial Model Comparison\n",
    "\n",
    "### Objective\n",
    "We train all the selected regression models using simple, initial parameter settings. This preliminary comparison helps us quickly identify which models perform better and are candidates for further hyperparameter tuning. In this phase, we:\n",
    "- Train models on three variants of our dataset:\n",
    "  - **Raw Data (No Scaling)**\n",
    "  - **Log + MinMax Scaled Data**\n",
    "  - **Log + Standard Scaled Data**\n",
    "- Note that ensemble learners (e.g., XGBoost, RandomForest) do not require scaling as they are tree-based models.\n",
    "\n",
    "### Parameter Settings & Explanations\n",
    "For a dataset with 20,000 records, we started with the following baseline parameter settings. Mostly the defaults are chosen(parameters when not specified are default) to provide a quick, balanced comparison across different model families. Later, based on the initial performance (measured by RMSE, MAE, R², etc.), we will decide which models warrant further tuning and which might be unsuitable for the task.\n",
    "\n",
    "\n",
    "## Linear Models\n",
    "\n",
    "For all linear models:\n",
    "\n",
    "### **Lasso, ElasticNet, LassoLars (L1 & L1/L2 Regularization)**\n",
    "- **alpha:** Controls the strength of regularization.\n",
    "  - *Higher values* enforce stronger regularization (more coefficients shrink to zero), while *lower values* reduce the penalty.\n",
    "  - **Setting:** `alpha = 0.01` (Prevents excessive feature elimination, ensuring useful features are retained).\n",
    "\n",
    "#### **ElasticNet-Specific**\n",
    "- **l1_ratio:** Defines the balance between L1 (sparsity) and L2 (stability).\n",
    "  - A value of `0.5` means an equal mix of L1 and L2 regularization.\n",
    "  - **Setting:** `l1_ratio = 0.5`.\n",
    "\n",
    "### **Ridge (L2 Regularization)**\n",
    "- **alpha:** Determines the strength of L2 regularization (helps stabilize the model when features are correlated).\n",
    "  - **Setting:** `alpha = 1.0` (A standard default that balances bias and variance).\n",
    "  \n",
    "### **SVR (Support Vector Regression)**\n",
    "- **C:** Controls the trade-off between a tight fit and model simplicity.\n",
    "  - *Higher C* leads to a closer fit, while *lower C* increases generalization.\n",
    "  - **Setting:** `C = 1.0` (A reasonable default that avoids overfitting).\n",
    "- **epsilon:** Defines a tolerance margin where small errors are ignored.\n",
    "  - *Larger epsilon* reduces sensitivity to minor errors.\n",
    "  - **Setting:** `epsilon = 0.1`.\n",
    "\n",
    "---\n",
    "\n",
    "## Ensemble Models\n",
    "\n",
    "Ensemble models do not require feature scaling and are evaluated separately.\n",
    "\n",
    "### **XGBoost (Gradient Boosting)**\n",
    "- **n_estimators:** Number of boosting rounds (trees).\n",
    "  - **Setting:** `n_estimators = 100` (Provides a reasonable start).\n",
    "- **learning_rate:** Step size for updates.\n",
    "  - **Setting:** `learning_rate = 0.1` (Commonly used for balanced performance).\n",
    "\n",
    "### **Random Forest**\n",
    "- **n_estimators:** Number of trees in the forest.\n",
    "  - **Setting:** `n_estimators = 100` (Provides good accuracy without excessive computation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of Models\n",
    "models = {\n",
    "    'Lasso': Lasso(alpha=0.01, random_state=42),\n",
    "    'ElasticNet': ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42),\n",
    "    'LassoLars': LassoLars(alpha=0.01),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"SVR (RBF)\": SVR(kernel=\"rbf\", C=1.0, epsilon=0.1),\n",
    "    \"SVR (Linear)\": SVR(kernel=\"linear\", C=1.0, epsilon=0.1)\n",
    "}\n",
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, target_transformed=False, dataset_label=\"\"):\n",
    "    print(f\"\\n🔍 Training and Evaluating Models on {dataset_label}...\\n\")\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # If the target was log-transformed, convert predictions and true values\n",
    "        # back to the original scale using np.expm1.\n",
    "        if target_transformed:\n",
    "            y_pred_original = np.expm1(y_pred)\n",
    "            y_true_original = np.expm1(y_test)\n",
    "        else:\n",
    "            y_pred_original = y_pred\n",
    "            y_true_original = y_test\n",
    "        \n",
    "        # Compute evaluation metrics on the original scale.\n",
    "        r2 = r2_score(y_true_original, y_pred_original)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_original, y_pred_original))\n",
    "        mae = mean_absolute_error(y_true_original, y_pred_original)\n",
    "        \n",
    "        results.append([name, r2, rmse, mae])\n",
    "    \n",
    "    return pd.DataFrame(results, columns=[\"Model\", \"R²\", \"RMSE\", \"MAE\"])\n",
    "\n",
    "# Run evaluations\n",
    "\n",
    "# 1. Raw Data (no scaling, no target transformation)\n",
    "results_unscaled = train_and_evaluate(\n",
    "    X_train_unscaled, X_test_unscaled, \n",
    "    y_train_raw, y_test_raw, \n",
    "    target_transformed=False, \n",
    "    dataset_label=\"Raw Data (No Scaling)\"\n",
    ")\n",
    "\n",
    "# 2. MinMax Scaled Data with log-transformed target\n",
    "results_minmax = train_and_evaluate(\n",
    "    X_train_minmax, X_test_minmax, \n",
    "    y_train_transformed, y_test_transformed, \n",
    "    target_transformed=True, \n",
    "    dataset_label=\"Log + MinMax Scaled Data\"\n",
    ")\n",
    "\n",
    "# 3. Standard Scaled Data with log-transformed target\n",
    "results_standard = train_and_evaluate(\n",
    "    X_train_standard, X_test_standard, \n",
    "    y_train_transformed, y_test_transformed, \n",
    "    target_transformed=True, \n",
    "    dataset_label=\"Log + Standard Scaled Data\"\n",
    ")\n",
    "\n",
    "# Print final comparisons\n",
    "print(\"\\n📌 Raw Data Model Results (No Scaling)\")\n",
    "print(results_unscaled)\n",
    "\n",
    "print(\"\\n📌 MinMax Scaled Model Results\")\n",
    "print(results_minmax)\n",
    "\n",
    "print(\"\\n📌 Standard Scaled Model Results (Z-Score)\")\n",
    "print(results_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Make copies to avoid modifying original data\n",
    "X_train_clean = X_train_unscaled.copy()\n",
    "X_test_clean = X_test_unscaled.copy()\n",
    "\n",
    "# Clean column names by replacing invalid characters (XGBoost requires valid column names)\n",
    "if hasattr(X_train_clean, 'columns'):  # Ensures it's a DataFrame\n",
    "    X_train_clean.columns = [col.replace('[', '_').replace(']', '_')\n",
    "                             .replace('<', '_').replace('>', '_') for col in X_train_clean.columns]\n",
    "    X_test_clean.columns = [col.replace('[', '_').replace(']', '_')\n",
    "                            .replace('<', '_').replace('>', '_') for col in X_test_clean.columns]\n",
    "\n",
    "# Train models and evaluate performance\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_clean, y_train_raw)  # Fit model on training data\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_log = model.predict(X_test_clean)\n",
    "    \n",
    "    # If target was log-transformed, apply inverse transformation\n",
    "    y_pred = y_pred_log  # Modify this if inverse transformation is needed\n",
    "    y_true = y_test_raw   # Ensure correct target data is used\n",
    "\n",
    "    # Compute metrics\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Print model performance\n",
    "    print(f\"{name} metrics:\")\n",
    "    print(f\"  R^2: {r2:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "\n",
    "## 📌 Raw Data Model Results (No Scaling)\n",
    "|      Model      |    R²    |       RMSE       |       MAE        |\n",
    "|---------------|----------|----------------|----------------|\n",
    "| Lasso        | 0.602784 |  72146.745365  | 49995.701905  |\n",
    "| ElasticNet   | 0.641424 |  68547.916557  | 49396.528646  |\n",
    "| LassoLars    | 0.602784 |  72146.739140  | 49995.698242  |\n",
    "| Ridge        | 0.609972 |  71490.970507  | 49887.781775  |\n",
    "| SVR (RBF)    | -0.048777 | 117231.706082  | 87345.328989  |\n",
    "| SVR (Linear) | 0.326650 |  93934.204923  | 69627.031320  |\n",
    "\n",
    "## 📌 MinMax Scaled Model Results\n",
    "|      Model      |      R²      |       RMSE       |       MAE        |\n",
    "|---------------|------------|----------------|----------------|\n",
    "| Lasso        | 0.572340   | 7.486051e+04  | 51487.212881  |\n",
    "| ElasticNet   | 0.567958   | 7.524305e+04  | 51950.806120  |\n",
    "| LassoLars    | 0.572340   | 7.486053e+04  | 51487.228162  |\n",
    "| Ridge        | -155.567604 | 1.432367e+06  | 70783.599247  |\n",
    "| SVR (RBF)    | 0.716328   | 6.096940e+04  | 40515.980631  |\n",
    "| SVR (Linear) | -11206.214972 | 1.211860e+07 | 239625.123031 |\n",
    "\n",
    "## 📌 Standard Scaled Model Results (Z-Score)\n",
    "|      Model      |      R²      |       RMSE       |       MAE        |\n",
    "|---------------|------------|----------------|----------------|\n",
    "| Lasso        | 0.627081   | 6.990543e+04  | 48223.883159  |\n",
    "| ElasticNet   | 0.639477   | 6.873371e+04  | 47348.991655  |\n",
    "| LassoLars    | 0.627020   | 6.991115e+04  | 48228.774122  |\n",
    "| Ridge        | -95.878676 | 1.126724e+06  | 64324.988797  |\n",
    "| **SVR (RBF)**    | 0.767188   | 5.523400e+04  | 36867.700983  |\n",
    "| SVR (Linear) | -19166.750397 | 1.584853e+07 | 296631.661033 |\n",
    "\n",
    "## 📌 Ensemble Model Results:\n",
    "\n",
    "|      Model       |    R²    |      RMSE       |       MAE        |\n",
    "|----------------|----------|----------------|----------------|\n",
    "| **XGBoost**       | 0.8280   | 47474.7816     | 31324.8338     |\n",
    "| **RandomForest**  | 0.8063   | 50378.8703     | 32339.3119     |\n",
    "\n",
    "Based on these results, the three best performing models on all three metrics are:\n",
    "\n",
    "- SVR(RBF) (on standard scaled data).\n",
    "- XGBoost\n",
    "- Randomforest\n",
    "\n",
    "The RMSE on the scaled data is very high and hard to compare, since when you use a log transformation on the target (via np.log1p) and then convert predictions back to the original scale with np.expm1, even small prediction errors in the log space can become very large once exponentiated.\n",
    "But since we have R² and MAE, we can compare those, and MAE remains in the original scale, making it more interpretable. RMSE, on the other hand, is more sensitive to large errors and can be distorted after transformation. Since MAE and RMSE typically follow similar trends, MAE is sufficient for evaluating model performance here.\n",
    "\n",
    "## 4.3.2 Hyperparameter Optimization and Model Refinement\n",
    "\n",
    "\n",
    "After establishing baseline performance in section 4.3.1, we now refine our models to improve prediction accuracy. Based on initial results, we have selected XGBoost and SVR (RBF) for fine tuning. The goal is to determine if adjusting key hyperparameters can further reduce errors (RMSE, MAE) and boost the R² score.\n",
    "\n",
    "Although Random Forest performed well, we will fine-tune only SVR (RBF) and XGBoost. XGBoost is likely the stronger tree-based model, making Random Forest redundant. SVR (RBF) offers a distinct, kernel-based approach, providing a meaningful comparison. \n",
    "\n",
    "## Methodology\n",
    "We will use **GridSearchCV** with 5-fold cross-validation on the training data to explore a predefined grid of hyperparameters for each model. This approach ensures an unbiased tuning process by using multiple training-validation splits.\n",
    "\n",
    "### Hyperparameters and Their Rationale\n",
    "\n",
    "**XGBoost:**\n",
    "- **n_estimators:** Number of trees in the ensemble.  \n",
    "  *Rationale:* More trees may capture more patterns but increase computational cost. We will test values such as 50, 100, and 150.\n",
    "- **learning_rate:** The step size shrinkage to prevent overfitting.  \n",
    "  *Rationale:* Lower values (e.g., 0.05, 0.1, 0.2) allow the model to learn gradually, potentially improving generalization.\n",
    "- **max_depth:** Maximum depth of each tree.  \n",
    "  *Rationale:* Controls model complexity. Shallower trees (e.g., depth 3, 5, or 7) can reduce overfitting.\n",
    "\n",
    "**SVR (RBF):**\n",
    "- **C:** Regularization parameter that controls the trade-off between training error and model complexity.  \n",
    "  *Rationale:* Lower C values enforce stronger regularization (simpler models), while higher C values allow a closer fit to the training data. We will try 0.1, 1.0, and 10.0.\n",
    "- **epsilon:** Defines the width of the margin in which no penalty is given to errors.  \n",
    "  *Rationale:* A smaller epsilon makes the model more sensitive to errors, whereas a larger epsilon provides a wider tolerance. We will explore values such as 0.01, 0.1, and 1.0.\n",
    "\n",
    "## Implementation Details\n",
    "Below is the code for hyperparameter tuning using 5-fold cross-validation with GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Tuning XGBoost\n",
    "# --------------------------\n",
    "xgb_model = XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, \n",
    "                               param_grid=xgb_param_grid, \n",
    "                               cv=5, \n",
    "                               scoring='neg_mean_squared_error')\n",
    "grid_search_xgb.fit(X_train_clean, y_train_raw)\n",
    "print(\"Best parameters for XGBoost:\", grid_search_xgb.best_params_)\n",
    "\n",
    "# --------------------------\n",
    "# Tuning SVR (RBF)\n",
    "# --------------------------\n",
    "svr_model = SVR(kernel=\"rbf\")\n",
    "svr_param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'epsilon': [0.01, 0.1, 1.0]\n",
    "}\n",
    "grid_search_svr = GridSearchCV(estimator=svr_model, \n",
    "                               param_grid=svr_param_grid, \n",
    "                               cv=5, \n",
    "                               scoring='neg_mean_squared_error')\n",
    "grid_search_svr.fit(X_train_standard, y_train_transformed)\n",
    "print(\"Best parameters for SVR (RBF):\", grid_search_svr.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Results and Comparison\n",
    "After tuning, we will compare the optimized models on our test set using evaluation metrics such as RMSE, MAE, and R². Visualizations (e.g., plots of parameter values versus performance metrics) can help illustrate the improvements from tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Comparing Tuned Models (XGBoost and SVR (RBF))\n",
    "\n",
    "# Evaluate Tuned XGBoost\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "y_pred_xgb = best_xgb.predict(X_test_clean)\n",
    "r2_xgb = r2_score(y_test_raw, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test_raw, y_pred_xgb))\n",
    "mae_xgb = mean_absolute_error(y_test_raw, y_pred_xgb)\n",
    "\n",
    "# Evaluate Tuned SVR (RBF)\n",
    "best_svr = grid_search_svr.best_estimator_\n",
    "y_pred_svr = best_svr.predict(X_test_standard)\n",
    "# Since the SVR was tuned on log-transformed target values, inverse-transform predictions and true values\n",
    "y_pred_svr_original = np.expm1(y_pred_svr)\n",
    "y_true_svr_original = np.expm1(y_test_transformed)\n",
    "r2_svr = r2_score(y_true_svr_original, y_pred_svr_original)\n",
    "rmse_svr = np.sqrt(mean_squared_error(y_true_svr_original, y_pred_svr_original))\n",
    "mae_svr = mean_absolute_error(y_true_svr_original, y_pred_svr_original)\n",
    "\n",
    "# Combine the results for a side-by-side comparison\n",
    "results_tuned = pd.DataFrame({\n",
    "    \"Model\": [\"XGBoost\", \"SVR (RBF)\"],\n",
    "    \"R²\": [r2_xgb, r2_svr],\n",
    "    \"RMSE\": [rmse_xgb, rmse_svr],\n",
    "    \"MAE\": [mae_xgb, mae_svr]\n",
    "})\n",
    "print(results_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "\n",
    "|      Model      |    R²    |      RMSE       |       MAE        |\n",
    "|---------------|----------|----------------|----------------|\n",
    "| XGBoost      | 0.839152 | 45910.471093   | 30029.313347   |\n",
    "| SVR (RBF)    | 0.773187 | 54517.743246   | 35738.362658   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "## 4.4 Assess Model\n",
    "\n",
    "**Objective:**\n",
    "The purpose of this phase is to evaluate the generated models to ensure they meet our technical and business success criteria. We assess the models based on key evaluation metrics (RMSE, MAE, R²), rank their performance, and interpret the results in business terms. This assessment also involves checking model plausibility and reliability.\n",
    "\n",
    "\n",
    "### 4.4.1 Model Comparison and Ranking\n",
    "\n",
    "In the California housing dataset, the relationship between house prices and input features is highly non-linear. Factors such as geographic location (e.g., proximity to the coast), local economic conditions, and population density interact in complex ways that simple linear models struggle to capture. These relationships often exhibit sudden jumps—such as sharp price increases near coastal areas—or plateau-like behaviors in regions with limited housing variability. \n",
    "\n",
    "As a result, simple linear models, including Lasso, Ridge, ElasticNet, and even Support Vector Regression (SVR) with a linear kernel, may fail to capture these intricate patterns. In contrast, non-linear models—such as tree-based ensembles (RandomForest, XGBoost) and SVR with an RBF kernel—are better suited for modeling complex interactions. Tree-based methods adaptively split the feature space, while kernel-based models like SVR with RBF can map input features into higher-dimensional spaces to capture non-linear relationships. This flexibility allows these methods to generate more accurate predictions.\n",
    "\n",
    "#### Model Performance Results\n",
    "\n",
    "Here are the results of our models (with RandomForest in its baseline, untuned state):\n",
    "\n",
    "| Model | R² | RMSE | MAE |\n",
    "|--------|------|---------|---------|\n",
    "| **XGBoost** | **0.8392** | **45,910** | **30,029** |\n",
    "| **RandomForest (baseline)** | 0.8063 | 50,379 | 32,339 |\n",
    "| **SVR (RBF)** | 0.7732 | 54,518 | 35,738 |\n",
    "\n",
    "\n",
    "\n",
    "## 4.4.2 Comparison to Data Mining Goals and Benchmark\n",
    "\n",
    "Our best-performing model, **XGBoost**, outperformed our initial performance targets and also exceeded the highest-performing model we found on Kaggle.\n",
    "\n",
    "**Our Performance Targets:**\n",
    "- **RMSE:** Less than **60,000** on the test set\n",
    "- **R²:** At least **0.7**\n",
    "- **MAE:** Less than **40,000**\n",
    "\n",
    "**Kaggle Model Performance [[3](https://www.kaggle.com/code/awnishsingh123/california-housing-prices-prediction#XGBoost-regression)]:**\n",
    "This was the found model on kaggle with the best results, which we outperformed:\n",
    "- **R²:** 0.826\n",
    "- **RMSE:** 48,144\n",
    "- **MAE:** 31,656\n",
    "\n",
    "**Our XGBoost Performance (Before Tuning):**\n",
    "- **R²:** 0.8280\n",
    "- **RMSE:** 47,474.78\n",
    "- **MAE:** 31,324.83\n",
    "\n",
    "**Key Takeaways:** \n",
    "- **Feature engineering had a positive impact**: The Kaggle model did not include deriving new features like `population_per_household`  or model tuning, which contributed to its lower performance.\n",
    "- **Tuning improved results further**: Even before tuning, our XGBoost model performed better than our targets and the Kaggle benchmark. After tuning, performance improved even more.\n",
    "- **XGBoost is the best model** for the California housing dataset, as it outperforms other models in capturing complex patterns and interactions.\n",
    "\n",
    "\n",
    "### 1. Interpretation in Business Terms\n",
    "\n",
    "- **High R² (~0.84)**  \n",
    "  This means our model accounts for roughly 84% of the variation in California housing prices. In practical real-estate terms, such a high R² suggests the model is capturing key drivers—like location, socioeconomic factors, and neighborhood characteristics—crucial for pricing decisions.  \n",
    "  For stakeholders such as investors or homebuyers, this metric conveys strong confidence that the model’s price estimates are well-grounded in observable market realities.\n",
    "\n",
    "- **RMSE (~ \\$46,000)**  \n",
    "  The model’s Root Mean Squared Error sits around \\$46,000, meaning it deviates from actual prices by this amount on average. In a market where median property values often exceed \\\\$500,000, an error margin of ~9% can still be considered practical for tasks like setting listing prices or performing initial investment analyses.\n",
    "\n",
    "- **MAE (~ \\$31,000)**  \n",
    "  The Mean Absolute Error of \\$31,000 indicates that, on average, the model’s predicted prices differ from actual prices by \\\\$31,000. This is especially useful for budgeting purposes and gauging the typical “day-to-day” variance between the model's predictions and the market.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Plausibility Checks\n",
    "\n",
    "- **Preliminary Range Checks**  \n",
    "  While we do not have definitive proof that the model is entirely free of implausible predictions, quick spot checks have shown most predictions remain within a realistic price range for California. As a further safeguard, outlier analyses can help identify and investigate any extreme estimates.\n",
    "\n",
    "- **Consistency Across Data Splits**  \n",
    "  The model was evaluated under multiple train/test splits, and it consistently achieved similar performance metrics (R², RMSE, MAE). This consistency reduces the likelihood of overfitting to particular subsets of the data or relying on spurious patterns.\n",
    "\n",
    "- **Reflection of Non-Linear Interactions**  \n",
    "  The superior performance of XGBoost over linear models aligns with known complexities in real-estate markets. Variables such as coastal proximity, population density, and median income rarely follow strictly linear relationships. XGBoost’s tree-based approach captures these interactions in a way that closely matches real-world pricing behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Feature Importance Insights\n",
    "\n",
    "Below is an XGBoost feature importance chart based on **Gain**, indicating which features most improve model accuracy.\n",
    "\n",
    "- **Most Significant Features**  \n",
    "  - **`ocean_INLAND`**: Dominates feature importance, confirming that proximity to coastal areas (or the lack thereof) dramatically influences property values in California.  \n",
    "  - **`median_income`**: Neighborhood income levels are critical predictors; higher-income areas often command higher home prices.\n",
    "\n",
    "- **Moderately Significant Features**  \n",
    "  - **`population_per_household`**: Reflects how densely people live in each household. Dense living conditions can push prices up or down, depending on the local economy and housing supply.  \n",
    "  - **`housing_median_age`**: Suggests that the age of residences plays a notable role, potentially linked to neighborhood desirability and renovation levels.\n",
    "\n",
    "- **Least Significant Features**\n",
    "    - A lot of other features does not influence the model a lot like `population` and `total_rooms`.\n",
    "    - While `population_per_household` does matter, population on itself does not matter, which shows the importance of the creativity of the data scientist to derive new features\n",
    "\n",
    "These results reinforce the notion that **location** factors (coastal vs. inland, neighborhood income) and **living density** dynamics shape California housing prices more strongly than raw counts of rooms or total population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Feature Importance für XGBoost\n",
    "def plot_feature_importance(model, X_test):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    xgb.plot_importance(model, importance_type=\"gain\", ax=plt.gca())\n",
    "    plt.title(\"Feature Importance (nach Gain)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Feature Importance Plot für XGBoost mit angepassten Variablen\n",
    "plot_feature_importance(best_xgb, X_test_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "\n",
    "# 5 Evaluation\n",
    "\n",
    "**Objective:**\n",
    "In this phase, we aim to determine how effectively our model meets the data mining goals and to assess its practical value—even though it is not tied to a direct commercial project. We also evaluate whether the model’s predictions make sense given known data limitations (e.g., the 1990 time period, the \\$500k cap).\n",
    "\n",
    "\n",
    "## 5.1 Evaluate Results\n",
    "Because this is an **educational** project rather than a commercial one, we do not have formal business success metrics. However, our data mining objective—building a model to predict the median house price for Californian census block groups—was fulfilled successfully. \n",
    "\n",
    "From a “potential business” perspective, **investors, homebuyers, and sellers** could apply the same methodology using **up-to-date data**, where an XGBoost approach might yield the strongest performance. That said, because the dataset is from 1990, the model itself has limited real-world utility for current housing markets.\n",
    "\n",
    "### Why is this model reliable for houses under \\$500k?\n",
    "If someone is asking \\$460k for a property and the model, given the relevant features, predicts \\$430k or \\$490k, then—given an MAE of around \\$30k—that list price is neither glaringly high nor unrealistically low. Essentially, the model’s moderate error range suggests that its estimate could be a reasonable benchmark for homes below the \\$500k threshold.\n",
    "\n",
    "**Limitation for Higher-End Properties**  \n",
    "All real-world valuations exceeding \\$500k are capped at \\$500k in the dataset, making it impossible for the model to learn distinctions above that ceiling (e.g., \\$700k vs. \\$1 million).\n",
    "\n",
    "\n",
    "### Answering Research Questions & Key Findings\n",
    "\n",
    "1. **Which features are most influential in determining California housing prices?**  \n",
    "   - **`ocean_INLAND`**  \n",
    "   - **`median_income`**  \n",
    "   - **`population_per_household`**  \n",
    "\n",
    "   In other words, housing density, income levels, and location drive prices more strongly than total rooms or other variables.\n",
    "\n",
    "3. **Which machine learning model provides the most accurate predictions?**  \n",
    "   - **Non-linear methods** stood out, particularly ensemble models such as **Random Forest** and **XGBoost**, with XGBoost achieving the best overall performance.\n",
    "\n",
    "5. **Can there be derived features that improve the model?**  \n",
    "   - Yes. For instance, **`population_per_household`** emerged as one of the most important predictors in the XGBoost model, showing how combined (engineered) features can strengthen predictions.\n",
    "\n",
    "7. **Which scaling method is the best for this dataset?**  \n",
    "   - It depends on the model. **Ensemble methods** (Random Forest, XGBoost) did not require scaling.  \n",
    "   - For **SVR (RBF)**, **standard scaling** performed better than min-max or no scaling.  \n",
    "   - In practice, this must be tested empirically, as seen in our own comparisons where standard scaling excelled in some cases, while other scenarios (e.g., ElasticNet) performed comparably or better without it.\n",
    "\n",
    "## 5.2 Review Process\n",
    "\n",
    "Although we do not plan to deploy this model, our review in Section 5.2 points to several potential follow-up actions to enhance the analysis and model performance:\n",
    "\n",
    "- **Create Additional Features**  \n",
    "  For example, calculate the distance to major cities or the coastline, cluster census blocks by region, or introduce interaction features (e.g., `median_income × population_per_household`). These engineered features could reveal hidden spatial or demographic patterns that basic attributes do not fully capture.\n",
    "\n",
    "- **Explore Alternative Ensemble Regression Methods**  \n",
    "  Beyond Random Forest and XGBoost, consider other ensemble learners.\n",
    "\n",
    "- **Investigate Missing Values in `total_bedrooms`**  \n",
    "  Delve into whether the absence of bedroom data follows any discernible pattern—perhaps relating to location, property type, or survey errors. Understanding why these values were missing in the first place could guide more targeted imputation strategies or alert us to potential biases in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\n",
    "\n",
    "[2] U.S. Census Bureau. _California Profile_. URL: [https://data.census.gov/profile/California?g=040XX00US06](https://data.census.gov/profile/California?g=040XX00US06).\n",
    "\n",
    "[3] Awnish Singh. _California Housing Prices Prediction_. Kaggle, 2024. URL: [https://www.kaggle.com/code/awnishsingh123/california-housing-prices-prediction](https://www.kaggle.com/code/awnishsingh123/california-housing-prices-prediction).\n",
    "\n",
    "[4] Faizan Ali. _California Housing Prices: EDA & Modeling_. Kaggle, 2024. URL: [https://www.kaggle.com/code/faizanali999/california-housing-prices-eda-modeling](https://www.kaggle.com/code/faizanali999/california-housing-prices-eda-modeling).\n",
    "\n",
    "[5] Josh Crotty. _California Housing Prices: RandomForestRegressor_. Kaggle, 2024. URL: [https://www.kaggle.com/code/joshcrotty/california-housing-prices-randomforestregressor](https://www.kaggle.com/code/joshcrotty/california-housing-prices-randomforestregressor).\n",
    "\n",
    "[6] Muhammet Ipıl. _Everything on Linear Regression: Model Evaluations_. Kaggle, 2024. URL: [https://www.kaggle.com/code/muhammetipil/everything-on-linear-regression#Model-Evaluations-](https://www.kaggle.com/code/muhammetipil/everything-on-linear-regression#Model-Evaluations-).\n",
    "\n",
    "[7] Varshita Nalluri. _Multiple Linear Regression_. Kaggle, 2024. URL: [https://www.kaggle.com/code/varshitanalluri/multiple-linear-regression](https://www.kaggle.com/code/varshitanalluri/multiple-linear-regression).\n",
    "\n",
    "[8] Analytics Vidhya. _Know the Best Evaluation Metrics for Your Regression Model_. URL: [https://www.analyticsvidhya.com/blog/2021/05/know-the-best-evaluation-metrics-for-your-regression-model/#h-types-of-regression-metrics](https://www.analyticsvidhya.com/blog/2021/05/know-the-best-evaluation-metrics-for-your-regression-model/#h-types-of-regression-metrics).\n",
    "\n",
    "[9] SAGE Journals. _Regression Model Evaluation Metrics_. URL: [https://journals.sagepub.com/doi/epub/10.1177/00045632211050531](https://journals.sagepub.com/doi/epub/10.1177/00045632211050531).\n",
    "\n",
    "[10] Analytics Vidhya. _Types of Categorical Data Encoding_. URL: [https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/](https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/).\n",
    "\n",
    "[11] Everton Gomede. _Feature Engineering for Geospatial Data: Navigating the Spatial Frontier_. Medium, 2024. URL: [https://medium.com/@evertongomede/feature-engineering-for-geospatial-data-navigating-the-spatial-frontier-4b6c8354eb2a](https://medium.com/@evertongomede/feature-engineering-for-geospatial-data-navigating-the-spatial-frontier-4b6c8354eb2a).\n",
    "\n",
    "[12] Scikit-Learn. _Machine Learning Map_. URL: [https://scikit-learn.org/stable/machine_learning_map.html](https://scikit-learn.org/stable/machine_learning_map.html).\n",
    "\n",
    "[13] Scikit-Learn. _Supervised Learning_. URL: [https://scikit-learn.org/stable/supervised_learning.html](https://scikit-learn.org/stable/supervised_learning.html).\n",
    "\n",
    "[14] Stack Exchange. _Least Angle Regression vs. Lasso_. URL: [https://stats.stackexchange.com/questions/4663/least-angle-regression-vs-lasso](https://stats.stackexchange.com/questions/4663/least-angle-regression-vs-lasso).\n",
    "\n",
    "[15] Neptune.ai. _Fighting Overfitting with L1 or L2 Regularization_. URL: [https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization).\n",
    "\n",
    "[16] Wikipedia. _Feature Scaling Methods_. URL: [https://en.wikipedia.org/wiki/Feature_scaling#Methods](https://en.wikipedia.org/wiki/Feature_scaling#Methods).\n",
    "\n",
    "[17] Vitalflux. _MinMaxScaler vs StandardScaler: Python Examples_. URL: [https://vitalflux.com/minmaxscaler-standardscaler-python-examples/#:~:text=Similarly%2C%20when%20dealing%20with%20non,the%20algorithm%20requires%20standardized%20features.](https://vitalflux.com/minmaxscaler-standardscaler-python-examples/#:~:text=Similarly%2C%20when%20dealing%20with%20non,the%20algorithm%20requires%20standardized%20features.).\n",
    "\n",
    "[18] SAGE Journals. _Regression Model Evaluation Metrics_. URL: [https://journals.sagepub.com/doi/epub/10.1177/00045632211050531](https://journals.sagepub.com/doi/epub/10.1177/00045632211050531).\n",
    "\n",
    "[19] Machine Learning Mastery. _Data Preparation Without Data Leakage_. URL: [https://machinelearningmastery.com/data-preparation-without-data-leakage/](https://machinelearningmastery.com/data-preparation-without-data-leakage/).\n",
    "\n",
    "[20] Machine Learning Mastery. _K-Fold Cross-Validation_. URL: [https://www.machinelearningmastery.com/k-fold-cross-validation/](https://www.machinelearningmastery.com/k-fold-cross-validation/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
